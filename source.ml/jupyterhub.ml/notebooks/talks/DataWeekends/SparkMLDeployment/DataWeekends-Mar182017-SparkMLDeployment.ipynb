{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who Am I?\n",
    "## Chris Fregly\n",
    "\n",
    "![Chris Fregly](https://s3.amazonaws.com/fluxcapacitor.com/img/fregly-300x300.png)\n",
    "\n",
    "![Chris Fregly Emoji](https://s3.amazonaws.com/fluxcapacitor.com/img/fregly-emoji-300x300.png)\n",
    "## Research Scientist, Founder @ **[PipelineIO](http://pipeline.io)** \n",
    "\n",
    "![PipelineIO](http://pipeline.io/images/pipeline-io-logo-shadow-210x186.png)\n",
    "\n",
    "## Video Series Author \"High Performance Tensorflow in Production\" @ [OReilly](http://oreilly.com) (Coming Soon)\n",
    "\n",
    "![OReilly Media](https://s3.amazonaws.com/fluxcapacitor.com/img/oreilly-logo.png)\n",
    "\n",
    "## Founder @ **[Advanced Spark and Tensorflow Meetup](http://http://www.meetup.com/Advanced-Spark-and-TensorFlow-Meetup/)**\n",
    "\n",
    "![Advanced Spark and Tensorflow Meetup](https://s3.amazonaws.com/fluxcapacitor.com/img/advanced-spark-tensorflow-meetup-logo-green.png)\n",
    "\n",
    "## **[Github Repo](https://github.com/fluxcapacitor/pipeline)**\n",
    "\t\n",
    "![Github Repo](https://s3.amazonaws.com/fluxcapacitor.com/img/pipeline-io-github-1000-stars.png)\n",
    "\n",
    "## **[DockerHub Repo](https://hub.docker.com/u/fluxcapacitor/)**\n",
    "\n",
    "![DockerHub](https://s3.amazonaws.com/fluxcapacitor.com/img/dockerhub-logo.png)\n",
    "\n",
    "## **[Slideshare](http://www.slideshare.net/cfregly)**\n",
    "\n",
    "![Slideshare](http://advancedspark.com/img/slideshare.png)\n",
    "\n",
    "## **[YouTube](https://www.youtube.com/playlist?list=PL7pBcJ870QHeNRBXdKirc4fdtbtbB5Xy-)**\n",
    "\n",
    "![YouTube](http://advancedspark.com/img/youtube-300x134.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Who Was I?\n",
    "## Software Engineer @ **Netflix**, **Databricks**, **IBM Spark Tech Center**\n",
    "\n",
    "![Netflix](http://pipeline.io/images/netflixoss-logo-white-295x55.png) \n",
    "\n",
    "![Databricks](http://pipeline.io/images/databricks-logo-350x69.png) \n",
    "\n",
    "![IBM Spark Tech Center](https://s3.amazonaws.com/fluxcapacitor.com/img/ibm-spark-logo-197x148.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Model Deployments\n",
    "## KeyValue\n",
    "ie. Recommendations\n",
    "\n",
    "In-memory: Redis, Memcache\n",
    "\n",
    "On-disk:  Cassandra, RocksDB\n",
    "\n",
    "First-class Servable in [Tensorflow Serving](https://github.com/tensorflow/serving/tree/master/tensorflow_serving/servables)\n",
    "\n",
    "## PMML\n",
    "It's Useful and [Well-Supported](https://github.com/jpmml)\n",
    "\n",
    "Apple, Cisco, [Airbnb](http://nerds.airbnb.com/architecting-machine-learning-system-risk/), HomeAway, etc\n",
    "\n",
    "Please Don't Re-build It - Reduce Your Technical Debt!\n",
    "\n",
    "![Regression Model PMML](https://s3.amazonaws.com/fluxcapacitor.com/img/regression-model-pmml2.png)\n",
    "\n",
    "## Native Code Generation (CPU and GPU)\n",
    "Hand-coded (Python + Pickling)\n",
    "\n",
    "Generate Java Code from PMML?\n",
    "\n",
    "![Regression Model Java Codegen](https://s3.amazonaws.com/fluxcapacitor.com/img/regression-model-codegen2.png)\n",
    "\n",
    "## Tensorflow Models\n",
    "[freeze_graph.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py): Combine Tensorflow Graph (Static) with Trained Weights (Checkpoints) into Single Deployable Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demos!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may need to Reconnect (more than Restart) the Kernel to pick up changes to these sett\n",
    "import os\n",
    "\n",
    "master = '--master spark://spark-master-2-1-0:7077'\n",
    "conf = '--conf spark.cores.max=1 --conf spark.executor.memory=512m'\n",
    "packages = '--packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.1'\n",
    "jars = '--jars /root/lib/jpmml-sparkml-package-1.0-SNAPSHOT.jar'\n",
    "py_files = '--py-files /root/lib/jpmml.py'\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = master \\\n",
    "  + ' ' + conf \\\n",
    "  + ' ' + packages \\\n",
    "  + ' ' + jars \\\n",
    "  + ' ' + py_files \\\n",
    "  + ' ' + 'pyspark-shell'\n",
    "\n",
    "print(os.environ['PYSPARK_SUBMIT_ARGS'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Spark ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 0**: Load Libraries and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "  .option(\"inferSchema\", \"true\").option(\"header\", \"true\") \\\n",
    "  .load(\"s3a://datapalooza/airbnb/airbnb.csv.bz2\")\n",
    "\n",
    "df.registerTempTable(\"df\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1**: Clean, Filter, and Summarize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df.filter(\"price >= 50 AND price <= 750 AND bathrooms > 0.0 AND bedrooms is not null\")\n",
    "\n",
    "df_filtered.registerTempTable(\"df_filtered\")\n",
    "\n",
    "df_final = spark.sql(\"\"\"\n",
    "    select\n",
    "        id,\n",
    "        city,\n",
    "        case when state in('NY', 'CA', 'London', 'Berlin', 'TX' ,'IL', 'OR', 'DC', 'WA')\n",
    "            then state\n",
    "            else 'Other'\n",
    "        end as state,\n",
    "        space,\n",
    "        cast(price as double) as price,\n",
    "        cast(bathrooms as double) as bathrooms,\n",
    "        cast(bedrooms as double) as bedrooms,\n",
    "        room_type,\n",
    "        host_is_super_host,\n",
    "        cancellation_policy,\n",
    "        cast(case when security_deposit is null\n",
    "            then 0.0\n",
    "            else security_deposit\n",
    "        end as double) as security_deposit,\n",
    "        price_per_bedroom,\n",
    "        cast(case when number_of_reviews is null\n",
    "            then 0.0\n",
    "            else number_of_reviews\n",
    "        end as double) as number_of_reviews,\n",
    "        cast(case when extra_people is null\n",
    "            then 0.0\n",
    "            else extra_people\n",
    "        end as double) as extra_people,\n",
    "        instant_bookable,\n",
    "        cast(case when cleaning_fee is null\n",
    "            then 0.0\n",
    "            else cleaning_fee\n",
    "        end as double) as cleaning_fee,\n",
    "        cast(case when review_scores_rating is null\n",
    "            then 80.0\n",
    "            else review_scores_rating\n",
    "        end as double) as review_scores_rating,\n",
    "        cast(case when square_feet is not null and square_feet > 100\n",
    "            then square_feet\n",
    "            when (square_feet is null or square_feet <=100) and (bedrooms is null or bedrooms = 0)\n",
    "            then 350.0\n",
    "            else 380 * bedrooms\n",
    "        end as double) as square_feet\n",
    "    from df_filtered\n",
    "\"\"\").persist()\n",
    "\n",
    "df_final.registerTempTable(\"df_final\")\n",
    "\n",
    "df_final.select(\"square_feet\", \"price\", \"bedrooms\", \"bathrooms\", \"cleaning_fee\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_final.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_final.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most popular cities\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        state,\n",
    "        count(*) as ct,\n",
    "        avg(price) as avg_price,\n",
    "        max(price) as max_price\n",
    "    from df_final\n",
    "    group by state\n",
    "    order by count(*) desc\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most expensive popular cities\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select \n",
    "        city,\n",
    "        count(*) as ct,\n",
    "        avg(price) as avg_price,\n",
    "        max(price) as max_price\n",
    "    from df_final\n",
    "    group by city\n",
    "    order by avg(price) desc\n",
    "\"\"\").filter(\"ct > 25\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 2**: Define Continous and Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_features = [\"bathrooms\", \\\n",
    "                       \"bedrooms\", \\\n",
    "                       \"security_deposit\", \\\n",
    "                       \"cleaning_fee\", \\\n",
    "                       \"extra_people\", \\\n",
    "                       \"number_of_reviews\", \\\n",
    "                       \"square_feet\", \\\n",
    "                       \"review_scores_rating\"]\n",
    "\n",
    "categorical_features = [\"room_type\", \\\n",
    "                        \"host_is_super_host\", \\\n",
    "                        \"cancellation_policy\", \\\n",
    "                        \"instant_bookable\", \\\n",
    "                        \"state\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 3**: Split Data into Training and Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[training_dataset, validation_dataset] = df_final.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 4**: Continous Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_feature_assembler = VectorAssembler(inputCols=continuous_features, outputCol=\"unscaled_continuous_features\")\n",
    "\n",
    "continuous_feature_scaler = StandardScaler(inputCol=\"unscaled_continuous_features\", outputCol=\"scaled_continuous_features\", \\\n",
    "                                           withStd=True, withMean=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 5**: Categorical Feature Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feature_indexers = [StringIndexer(inputCol=x, \\\n",
    "                                              outputCol=\"{}_index\".format(x)) \\\n",
    "                                for x in categorical_features]\n",
    "\n",
    "categorical_feature_one_hot_encoders = [OneHotEncoder(inputCol=x.getOutputCol(), \\\n",
    "                                                      outputCol=\"oh_encoder_{}\".format(x.getOutputCol() )) \\\n",
    "                                        for x in categorical_feature_indexers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 6**: Assemble our Features and Feature Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols_lr = [x.getOutputCol() \\\n",
    "                   for x in categorical_feature_one_hot_encoders]\n",
    "feature_cols_lr.append(\"scaled_continuous_features\")\n",
    "\n",
    "feature_assembler_lr = VectorAssembler(inputCols=feature_cols_lr, \\\n",
    "                                       outputCol=\"features_lr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 7**: Train a Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression = LinearRegression(featuresCol=\"features_lr\", \\\n",
    "                                     labelCol=\"price\", \\\n",
    "                                     predictionCol=\"price_prediction\", \\\n",
    "                                     maxIter=10, \\\n",
    "                                     regParam=0.3, \\\n",
    "                                     elasticNetParam=0.8)\n",
    "\n",
    "estimators_lr = \\\n",
    "  [continuous_feature_assembler, continuous_feature_scaler] \\\n",
    "  + categorical_feature_indexers + categorical_feature_one_hot_encoders \\\n",
    "  + [feature_assembler_lr] + [linear_regression]\n",
    "\n",
    "pipeline = Pipeline(stages=estimators_lr)\n",
    "\n",
    "pipeline_model = pipeline.fit(training_dataset)\n",
    "\n",
    "print(pipeline_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 8**:  Serialize PipelineModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jpmml import toPMMLBytes\n",
    "\n",
    "model_bytes = toPMMLBytes(spark, training_dataset, pipeline_model)\n",
    "\n",
    "print(pmmlBytes.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Push Model to Live, Running Spark ML Model Server (Mutable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "namespace = 'default'\n",
    "model_name = 'airbnb'\n",
    "version = '1'\n",
    "update_url = 'http://prediction-pmml-aws.demo.pipeline.io/update-pmml-model/%s/%s/%s' % (namespace, model_name, version)\n",
    "\n",
    "update_headers = {}\n",
    "update_headers['Content-type'] = 'application/xml'\n",
    "\n",
    "req = urllib.request.Request(update_url, \\\n",
    "                             headers=update_headers, \\\n",
    "                             data=model_bytes)\n",
    "\n",
    "resp = urllib.request.urlopen(req)\n",
    "\n",
    "print(resp.status) # Should return Http Status 200 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10:  Evalute Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "import json\n",
    "\n",
    "namespace = 'default'\n",
    "model_name = 'airbnb'\n",
    "version = '1'\n",
    "evaluate_url = 'http://prediction-pmml-aws.demo.pipeline.io/evaluate-pmml-model/%s/%s/%s' % (namespace, model_name, version)\n",
    "\n",
    "evaluate_headers = {}\n",
    "evaluate_headers['Content-type'] = 'application/json'\n",
    "\n",
    "input_params = '{\"bathrooms\":5.0, \\\n",
    "                 \"bedrooms\":4.0, \\\n",
    "                 \"security_deposit\":175.00, \\\n",
    "                 \"cleaning_fee\":25.0, \\\n",
    "                 \"extra_people\":1.0, \\\n",
    "                 \"number_of_reviews\": 2.0, \\\n",
    "                 \"square_feet\": 250.0, \\\n",
    "                 \"review_scores_rating\": 2.0, \\\n",
    "                 \"room_type\": \"Entire home/apt\", \\\n",
    "                 \"host_is_super_host\": \"0.0\", \\\n",
    "                 \"cancellation_policy\": \"flexible\", \\\n",
    "                 \"instant_bookable\": \"1.0\", \\\n",
    "                 \"state\": \"CA\"}' \n",
    "encoded_input_params = input_params.encode('utf-8')\n",
    "\n",
    "req = urllib.request.Request(evaluate_url, \\\n",
    "                             headers=evaluate_headers, \\\n",
    "                             data=encoded_input_params)\n",
    "\n",
    "resp = urllib.request.urlopen(req)\n",
    "\n",
    "print(resp.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Demos!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Java-based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Java-based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "\n",
    "sourceBytes = '                                                      \\n\\\n",
    "  private String str;                                                \\n\\\n",
    "                                                                     \\n\\\n",
    "  public void initialize(Map<String, Object> args) {                 \\n\\\n",
    "  }                                                                  \\n\\\n",
    "                                                                     \\n\\\n",
    "  public Object predict(Map<String, Object> inputs) {                \\n\\\n",
    "      String id = (String)inputs.get(\"id\");                          \\n\\\n",
    "                                                                     \\n\\\n",
    "      return id.equals(\"21619\");                                     \\n\\\n",
    "  }                                                                  \\n\\\n",
    "'.encode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Java-based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/* 001 */\n",
      "/* 002 */ private String str;\n",
      "/* 003 */\n",
      "/* 004 */ public void initialize(Map<String, Object> args) {\n",
      "/* 005 */ }\n",
      "/* 006 */\n",
      "/* 007 */ public Object predict(Map<String, Object> inputs) {\n",
      "/* 008 */   String id = (String)inputs.get(\"id\");\n",
      "/* 009 */\n",
      "/* 010 */   return id.equals(\"21619\");\n",
      "/* 011 */ }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "\n",
    "namespace = 'default'\n",
    "model_name = 'java_equals'\n",
    "version = '1'\n",
    "\n",
    "update_url = 'http://prediction-java-aws.demo.pipeline.io/update-java/%s/%s/%s' % (namespace, model_name, version)\n",
    "\n",
    "update_headers = {}\n",
    "update_headers['Content-type'] = 'text/plain'\n",
    "\n",
    "req = request.Request(\"%s\" % update_url, headers=update_headers, data=sourceBytes)\n",
    "resp = request.urlopen(req)\n",
    "\n",
    "generated_code = resp.read()\n",
    "print(generated_code.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Java-based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'false'\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "\n",
    "namespace = 'default'\n",
    "model_name = 'java_equals'\n",
    "version = '1'\n",
    "\n",
    "evaluate_url = 'http://prediction-java-aws.demo.pipeline.io/evaluate-java/%s/%s/%s' % (namespace, model_name, version)\n",
    "\n",
    "evaluate_headers = {}\n",
    "evaluate_headers['Content-type'] = 'application/json'\n",
    "input_params = '{\"id\":\"21618\"}' \n",
    "encoded_input_params = input_params.encode('utf-8')\n",
    "\n",
    "req = request.Request(evaluate_url, headers=evaluate_headers, data=encoded_input_params)\n",
    "resp = request.urlopen(req)\n",
    "\n",
    "print(resp.read()) # Should return false "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'true'\n"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "\n",
    "namespace = 'default'\n",
    "model_name = 'java_equals'\n",
    "version = '1'\n",
    "evaluate_url = 'http://prediction-java-aws.demo.pipeline.io/evaluate-java/%s/%s/%s' % (namespace, model_name, version)\n",
    "\n",
    "evaluate_headers = {}\n",
    "evaluate_headers['Content-type'] = 'application/json'\n",
    "input_params = '{\"id\":\"21619\"}' \n",
    "encoded_input_params = input_params.encode('utf-8')\n",
    "\n",
    "req = request.Request(evaluate_url, headers=evaluate_headers, data=encoded_input_params)\n",
    "resp = request.urlopen(req)\n",
    "\n",
    "print(resp.read()) # Should return true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Scikit-Learn Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn_pandas\n",
    "!pip install git+https://github.com/jpmml/sklearn2pmml.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Scikit-Learn Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PMMLPipeline(steps=[('classifier', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=None, splitter='best'))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import urllib.parse\n",
    "import json\n",
    "\n",
    "from sklearn.datasets import load_diabetes,load_iris\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error as mse, r2_score\n",
    "from sklearn2pmml import PMMLPipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn2pmml import sklearn2pmml\n",
    "\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(iris.data,columns=iris.feature_names)\n",
    "iris_df['Species'] = iris.target\n",
    "iris_pipeline = PMMLPipeline([\n",
    "    (\"classifier\", DecisionTreeClassifier())\n",
    "])\n",
    "iris_pipeline.fit(iris_df[iris_df.columns.difference([\"Species\"])], iris_df[\"Species\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Serialize Scikit-Learn Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn2pmml(iris_pipeline, \"DecisionTreeIris.pmml\", with_repr = True)\n",
    "model_bytes = bytearray(open('DecisionTreeIris.pmml', 'rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Scikit-Learn Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import urllib.parse\n",
    "\n",
    "namespace = 'default'\n",
    "model_name = 'iris'\n",
    "version = '1'\n",
    "\n",
    "update_url = 'http://prediction-pmml-aws.demo.pipeline.io/update-pmml-model/%s/%s/%s' % (namespace, model_name, version)\n",
    "\n",
    "update_headers = {}\n",
    "update_headers[\"Content-type\"] = \"application/xml\"\n",
    "\n",
    "req = urllib.request.Request(update_url, headers=update_headers, data=model_bytes)\n",
    "\n",
    "resp = urllib.request.urlopen(req)\n",
    "print(resp.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\"results\":[[{\\'Species\\': \\'NodeScoreDistribution{result=0, probability_entries=[0=1.0, 1=0.0, 2=0.0], entityId=2, confidence_entries=[]}\\'}]]}'\n"
     ]
    }
   ],
   "source": [
    "namespace = 'default'\n",
    "model_name = 'iris'\n",
    "version = '1'\n",
    "\n",
    "evaluate_url = 'http://prediction-pmml-aws.demo.pipeline.io/evaluate-pmml-model/%s/%s/%s' % (namespace, model_name, version)\n",
    "evaluate_headers = {}\n",
    "evaluate_headers['Content-type'] = 'application/json'\n",
    "\n",
    "input_params = iris_df.ix[0,:-1].to_json()\n",
    "encoded_input_params = input_params.encode('utf-8')\n",
    "\n",
    "req = urllib.request.Request(evaluate_url, headers=evaluate_headers, data=encoded_input_params)\n",
    "resp = urllib.request.urlopen(req)\n",
    "\n",
    "print(resp.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring Your Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netflix Microservices [Dashboard](http://hystrix.demo.pipeline.io/hystrix-dashboard/monitor/monitor.html?streams=%5B%7B%22name%22%3A%22Predictions%20-%20AWS%22%2C%22stream%22%3A%22http%3A%2F%2Fturbine-aws.demo.pipeline.io%2Fturbine.stream%22%2C%22auth%22%3A%22%22%2C%22delay%22%3A%22%22%7D%2C%7B%22name%22%3A%22Predictions%20-%20GCP%22%2C%22stream%22%3A%22http%3A%2F%2Fturbine-gcp.demo.pipeline.io%2Fturbine.stream%22%2C%22auth%22%3A%22%22%2C%22delay%22%3A%22%22%7D%5D) (Hystrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "html = '<iframe width=1200px height=500px src=\"http://hystrix.demo.pipeline.io/hystrix-dashboard/monitor/monitor.html?streams=%5B%7B%22name%22%3A%22Predictions%20-%20AWS%22%2C%22stream%22%3A%22http%3A%2F%2Fturbine-aws.demo.pipeline.io%2Fturbine.stream%22%2C%22auth%22%3A%22%22%2C%22delay%22%3A%22%22%7D%2C%7B%22name%22%3A%22Predictions%20-%20GCP%22%2C%22stream%22%3A%22http%3A%2F%2Fturbine-gcp.demo.pipeline.io%2Fturbine.stream%22%2C%22auth%22%3A%22%22%2C%22delay%22%3A%22%22%7D%5D\">'\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grafana + Prometheus [Dashboard](http://grafana.demo.pipeline.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "html = '<iframe width=1200px height=500px src=\"http://grafana.demo.pipeline.io\">'\n",
    "display(HTML(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load-Test Your Model Servers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run JMeter Tests from Local Laptop (Limited by Laptop Performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Headless JMeter Tests from Training Clusters in Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Spark ML - Airbnb\n",
    "!kubectl create --context=awsdemo -f /root/pipeline/loadtest.ml/loadtest-aws-airbnb-rc.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Codegen - Java - Simple\n",
    "!kubectl create --context=awsdemo -f /root/pipeline/loadtest.ml/loadtest-aws-equals-rc.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tensorflow AI - Tensorflow Serving - Simple \n",
    "!kubectl create --context=awsdemo -f /root/pipeline/loadtest.ml/loadtest-aws-minimal-rc.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End Load Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!kubectl delete --context=awsdemo rc loadtest-aws-airbnb\n",
    "!kubectl delete --context=awsdemo rc loadtest-aws-equals\n",
    "!kubectl delete --context=awsdemo rc loadtest-aws-minimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rolling Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!kubectl rolling-update prediction-tensorflow --context=awsdemo --image-pull-policy=Always --image=fluxcapacitor/prediction-tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PipelineIO Premium Edition\n",
    "\n",
    "![PipelineIO](http://pipeline.io/images/pipeline-io-logo-shadow-210x186.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A/B and Multi-armed Bandit Testing\n",
    "\n",
    "![A/B Testing](http://pipeline.io/images/ab-testing-170x100.png)\n",
    "![Multi-armed Bandit](http://pipeline.io/images/multi-armed-bandit-101x100.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous, Hybrid-Cloud Deployments\n",
    "![AWS](http://pipeline.io/images/aws-logo-185x73.png)\n",
    "![Google](http://pipeline.io/images/gce-logo-190x90.png)\n",
    "![Azure](http://pipeline.io/images/azure-logo-200x103.png)\n",
    "![Kubernetes](http://pipeline.io/images/kubernetes-logo-200x171.png)\n",
    "![Docker](http://pipeline.io/images/docker-logo-150x126.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Model Training and Deploying\n",
    "\n",
    "![Kafka](http://pipeline.io/images/kafka-logo-wide-219x98.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU-based Deployments\n",
    "\n",
    "![CUDA](http://pipeline.io/images/nvidia-cuda-338x181.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
