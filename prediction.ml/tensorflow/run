#!/bin/bash

#if [ ! -f ~/store ]; then
#  cd ~ && ln -s /root/volumes/source.ml/prediction.ml/tensorflow/store
#fi

#cd ~
#$TENSORFLOW_SERVING_HOME/bazel-bin/tensorflow_serving/model_servers/tensorflow_model_server --port=9000 --model_name=tensorflow_minimal --model_base_path=/root/store/default/tensorflow_minimal/export --enable_batching=true --file_system_poll_wait_seconds=120 &

#cd ~
#java -Djava.security.egd=file:/dev/./urandom -Djava.library.path=lib/jni/ -cp lib/libtensorflow-1.0.0-PREVIEW1.jar:target/tensorflow-prediction-client-1.0-SNAPSHOT.jar -jar lib/sbt-launch.jar "run-main com.advancedspark.serving.prediction.tensorflow.PredictionServiceMain"

# Args:
#  $1: port number (int)
#  $2: model_name (anything)
#  $3: /<namespace>/<model_name>/<version> (path above all the /<version>/ paths)
#  $4: request batching (true|false)

scripts/serve $PIO_MODEL_SERVER_PORT $PIO_MODEL_NAME $STORE_HOME/$PIO_MODEL_NAMESPACE/$PIO_MODEL_NAME/ $PIO_MODEL_SERVER_REQUEST_BATCHING 

scripts/http_grpc_proxy $PIO_MODEL_SERVER_HTTP_GRPC_PROXY_PORT $PIO_MODEL_SERVER_PORT 
